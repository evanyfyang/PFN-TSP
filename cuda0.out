nohup: ignoring input
Starting TSP training (nodes: 20-20)...
Using GPU: 0
=== Training Mode ===
Starting TSP model training...
Parameters: Namespace(emsize=256, nhid=256, nlayers=3, nhead=8, dropout=0.1, epochs=20, steps_per_epoch=100, batch_size=32, lr=0.0001, min_nodes=20, max_nodes=20, test_size=5, save_dir='./saved_models', cuda_device='cuda', train=True, model_path=None, decoding_strategy='greedy')
Training TSP model on cuda with 256 embedding size
Using cuda device
init dist
Not using distributed
Using 32 processes for TSP solving
Style definition of first 3 examples: None
Initialized decoder for standard with (None, 1)  and nout 1
Using a Transformer with 2.32 M parameters
Training Epoch:   0%|          | 0/100 [00:00<?, ?it/s]Training Epoch:   1%|          | 1/100 [00:01<02:00,  1.22s/it]Training Epoch:   1%|          | 1/100 [00:20<02:00,  1.22s/it, data_time=1.23, step_time=18.9, mean_loss=4.81]Training Epoch:   2%|▏         | 2/100 [00:21<20:40, 12.66s/it, data_time=1.23, step_time=18.9, mean_loss=4.81]Training Epoch:   2%|▏         | 2/100 [00:40<20:40, 12.66s/it, data_time=1.81, step_time=19, mean_loss=4.78]  Training Epoch:   3%|▎         | 3/100 [00:42<26:27, 16.37s/it, data_time=1.81, step_time=19, mean_loss=4.78]Training Epoch:   3%|▎         | 3/100 [01:02<26:27, 16.37s/it, data_time=1.76, step_time=19.7, mean_loss=4.79]Training Epoch:   4%|▍         | 4/100 [01:04<29:28, 18.42s/it, data_time=1.76, step_time=19.7, mean_loss=4.79]Training Epoch:   4%|▍         | 4/100 [01:22<29:28, 18.42s/it, data_time=1.91, step_time=18.6, mean_loss=4.77]Training Epoch:   5%|▌         | 5/100 [01:24<30:22, 19.19s/it, data_time=1.91, step_time=18.6, mean_loss=4.77]Training Epoch:   5%|▌         | 5/100 [01:44<30:22, 19.19s/it, data_time=1.98, step_time=19.6, mean_loss=4.78]Training Epoch:   6%|▌         | 6/100 [01:46<31:13, 19.93s/it, data_time=1.98, step_time=19.6, mean_loss=4.78]Training Epoch:   6%|▌         | 6/100 [02:04<31:13, 19.93s/it, data_time=1.76, step_time=17.9, mean_loss=4.77]