nohup: ignoring input
Starting TSP training (nodes: 10-15)...
Using GPU: 2
=== Training Mode ===
Starting TSP model training...
Parameters: Namespace(emsize=256, nhid=256, nlayers=3, nhead=8, dropout=0.1, epochs=20, steps_per_epoch=100, batch_size=32, lr=0.0001, min_nodes=10, max_nodes=15, test_size=5, save_dir='./saved_models', cuda_device='cuda', train=True, model_path=None, decoding_strategy='greedy')
Training TSP model on cuda with 256 embedding size
Using cuda device
init dist
Not using distributed
Using 32 processes for TSP solving
Style definition of first 3 examples: None
Initialized decoder for standard with (None, 1)  and nout 1
Using a Transformer with 2.31 M parameters
Training Epoch:   0%|          | 0/100 [00:00<?, ?it/s]Training Epoch:   1%|          | 1/100 [00:00<01:13,  1.34it/s]Training Epoch:   1%|          | 1/100 [00:06<01:13,  1.34it/s, data_time=0.748, step_time=6.19, mean_loss=10]Training Epoch:   2%|▏         | 2/100 [00:08<07:36,  4.66s/it, data_time=0.748, step_time=6.19, mean_loss=10]Training Epoch:   2%|▏         | 2/100 [00:10<07:36,  4.66s/it, data_time=1.21, step_time=2.78, mean_loss=9.97]Training Epoch:   3%|▎         | 3/100 [00:12<07:03,  4.37s/it, data_time=1.21, step_time=2.78, mean_loss=9.97]Training Epoch:   3%|▎         | 3/100 [00:16<07:03,  4.37s/it, data_time=1.23, step_time=4.17, mean_loss=9.91]Training Epoch:   4%|▍         | 4/100 [00:17<07:45,  4.84s/it, data_time=1.23, step_time=4.17, mean_loss=9.91]Training Epoch:   4%|▍         | 4/100 [00:23<07:45,  4.84s/it, data_time=1.41, step_time=5.44, mean_loss=9.97]