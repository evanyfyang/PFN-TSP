# TSP Model Training and Evaluation

This document describes how to use the `train_and_evaluate_tsp.py` script to train and evaluate a Prior-Focused Network (PFN) model for solving the Traveling Salesman Problem (TSP).

## Overview

The script implements the following functionalities:

1. Training a PFN model for TSP problems
2. Using the trained model to predict TSP tours
3. Comparing the PFN model with OR-Tools solutions generated by TSPDataLoader
4. Visualizing comparison results and saving statistical data

## Requirements

- Python 3.6+
- PyTorch 1.7+
- Matplotlib
- NumPy
- Other PFN project dependencies

## Usage

### Basic Usage

```bash
python train_and_evaluate_tsp.py
```

This trains and evaluates the model using default parameters.

### Parameter Adjustment

You can adjust the training and evaluation process using command line arguments:

```bash
python train_and_evaluate_tsp.py --emsize 256 --nlayers 6 --epochs 10 --batch_size 64 --min_nodes 15 --max_nodes 30
```

### Main Parameters

- `--emsize`: Embedding dimension size (default: 128)
- `--nhid`: Hidden layer dimension (default: 128)
- `--nlayers`: Number of Transformer layers (default: 4)
- `--nhead`: Number of attention heads (default: 4)
- `--dropout`: Dropout rate (default: 0.1)
- `--epochs`: Number of training epochs (default: 5)
- `--steps_per_epoch`: Steps per epoch (default: 100)
- `--batch_size`: Batch size (default: 32)
- `--lr`: Learning rate (default: 1e-4)
- `--min_nodes`: Minimum number of nodes in TSP (default: 10)
- `--max_nodes`: Maximum number of nodes in TSP (default: 20)
- `--test_size`: Number of test instances (default: 20)
- `--save_dir`: Directory to save models (default: ./saved_models)
- `--device`: Computing device (default: automatically selects cuda or cpu)

## Script Improvements

The latest version of the script includes the following optimizations:

1. **Simplified Testing Workflow**: Directly uses solutions generated by TSPDataLoader through OR-Tools, instead of re-invoking the OR-Tools solver during evaluation
2. **Improved Efficiency**: Eliminates redundant OR-Tools solution computation time, making the evaluation process more efficient
3. **Consistency**: Ensures that the problem instance generation method is identical between training and evaluation

## Output Description

The script produces the following outputs:

1. **Training Logs**: Shows training progress and loss values
2. **Saved Model**: Saves the trained model in the directory specified by `--save_dir`
3. **Comparison Visualization**: Randomly selects a test instance to create a comparison between PFN and OR-Tools solutions
4. **Evaluation Results**: Prints statistical data to the console, including:
   - Average path length comparison
   - Relative gap analysis
   - PFN win rate
   - PFN processing time analysis
5. **Results Data**: Saves detailed evaluation results as an NPZ file

## Results Analysis

The evaluation results compare PFN and OR-Tools in the following aspects:

- **Solution Quality**: Compares solution quality through path length
- **Computational Efficiency**: Analyzes PFN prediction speed
- **Scale Adaptability**: Performance changes with different numbers of nodes

## Example Output

```
Starting TSP model training...
...
Training completed in 342.15 seconds, model saved to ./saved_models/tsp_model_20230601_123045.pt
...
Generating test instances...
Processing test instance 1/20...
Instance 1: PFN distance=4.2356, OR-Tools distance=3.8765
PFN processing time: 0.0234 seconds
...
Comparison plot saved to tsp_comparison.png

===== Evaluation Results =====
Average path length: PFN=4.8765, OR-Tools=4.3456
Average relative gap: 12.25%
Maximum relative gap: 25.67%
Minimum relative gap: 3.45%
PFN win rate: 15.00%
PFN average processing time: 0.0321 seconds
Results saved to ./saved_models/tsp_results_20230601_124512.npz
```

## Advanced Usage

### 1. Loading a Pre-trained Model for Evaluation

You can modify the script to load a pre-trained model and only run the evaluation part:

```python
# Load pre-trained model
model = TransformerModel(...)  # Create model structure
model.load_state_dict(torch.load('path_to_model.pt'))

# Generate test instances with reference solutions
test_instances, ortools_solutions = generate_test_instances_with_ortools(...)

# Evaluation only
results = evaluate_and_compare(model, test_instances, ortools_solutions)
```

### 2. Analyzing Performance on Different Problem Scales

You can set different node ranges to analyze the model's performance on problems of different scales:

```bash
# Small-scale problems
python train_and_evaluate_tsp.py --min_nodes 5 --max_nodes 15

# Large-scale problems
python train_and_evaluate_tsp.py --min_nodes 30 --max_nodes 50
``` 